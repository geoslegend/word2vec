{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Word Vectors from Sherlock Holmes Series\n",
    "Patrick Coady (pcoady@alum.mit.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordvector import WordVector\n",
    "from windowmodel import WindowModel\n",
    "import docload\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Books, Build Dictionary & Convert Books to Integer Vector\n",
    "Start with these 3 books (all written by Sir Arthor Conan Doyle):\n",
    "1. The Adventures of Sherlock Holmes\n",
    "2. The Hound of the Baskervilles\n",
    "3. The Sign of the Four\n",
    "\n",
    "Load the books and build a dictionary of all unique words. The dictionary maps each unique word to an integer. All words are converted to lower case. And punctuation are treated as words (i.e. \" , . ? and !). If the size of the book vocabulary exceeds the pre-set limit (**vocab_size**), then the most infrequent words are mapped to the last integer in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded and processed: 24080 lines, 244986 words.\n"
     ]
    }
   ],
   "source": [
    "files = ['../data/adventures_of_sherlock_holmes.txt',\n",
    "        '../data/hound_of_the_baskervilles.txt',\n",
    "        '../data/sign_of_the_four.txt']\n",
    "word_array, dictionary, num_lines, num_words = docload.build_word_array(\n",
    "    files, vocab_size=50000, gutenberg=True)\n",
    "\n",
    "print('Document loaded and processed: {} lines, {} words.'\n",
    "      .format(num_lines, num_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Architecture\n",
    "![](notebook_images/NN_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training set ...\n",
      "Training set built.\n",
      "Model built. Vocab size = 11750. Document length = 244986 words.\n",
      "Training ...\n",
      "epoch 1: total batches = 6890. train loss = 39.38, val loss = 9.70\n",
      "epoch 2: total batches = 13780. train loss = 7.58, val loss = 6.80\n",
      "epoch 3: total batches = 20670. train loss = 6.68, val loss = 6.55\n",
      "epoch 4: total batches = 27560. train loss = 6.05, val loss = 5.73\n",
      "epoch 5: total batches = 34450. train loss = 5.38, val loss = 4.89\n",
      "epoch 6: total batches = 41340. train loss = 4.59, val loss = 4.31\n",
      "epoch 7: total batches = 48230. train loss = 4.15, val loss = 3.98\n",
      "epoch 8: total batches = 55120. train loss = 3.91, val loss = 3.79\n",
      "epoch 9: total batches = 62010. train loss = 3.76, val loss = 3.68\n",
      "epoch 10: total batches = 68900. train loss = 3.65, val loss = 3.58\n",
      "epoch 11: total batches = 75790. train loss = 3.57, val loss = 3.51\n",
      "epoch 12: total batches = 82680. train loss = 3.50, val loss = 3.43\n",
      "epoch 13: total batches = 89570. train loss = 3.44, val loss = 3.39\n",
      "epoch 14: total batches = 96460. train loss = 3.38, val loss = 3.33\n",
      "epoch 15: total batches = 103350. train loss = 3.33, val loss = 3.29\n",
      "epoch 16: total batches = 110240. train loss = 3.29, val loss = 3.24\n",
      "epoch 17: total batches = 117130. train loss = 3.25, val loss = 3.21\n",
      "epoch 18: total batches = 124020. train loss = 3.21, val loss = 3.18\n",
      "epoch 19: total batches = 130910. train loss = 3.18, val loss = 3.15\n",
      "epoch 20: total batches = 137800. train loss = 3.14, val loss = 3.11\n",
      "epoch 21: total batches = 144690. train loss = 3.11, val loss = 3.07\n",
      "epoch 22: total batches = 151580. train loss = 3.08, val loss = 3.06\n",
      "epoch 23: total batches = 158470. train loss = 3.05, val loss = 3.03\n",
      "epoch 24: total batches = 165360. train loss = 3.03, val loss = 3.01\n",
      "epoch 25: total batches = 172250. train loss = 3.00, val loss = 2.99\n",
      "epoch 26: total batches = 179140. train loss = 2.98, val loss = 2.96\n",
      "epoch 27: total batches = 186030. train loss = 2.96, val loss = 2.94\n",
      "epoch 28: total batches = 192920. train loss = 2.94, val loss = 2.92\n",
      "epoch 29: total batches = 199810. train loss = 2.91, val loss = 2.90\n",
      "epoch 30: total batches = 206700. train loss = 2.89, val loss = 2.88\n",
      "epoch 31: total batches = 213590. train loss = 2.87, val loss = 2.86\n",
      "epoch 32: total batches = 220480. train loss = 2.86, val loss = 2.84\n",
      "epoch 33: total batches = 227370. train loss = 2.84, val loss = 2.82\n",
      "epoch 34: total batches = 234260. train loss = 2.81, val loss = 2.82\n",
      "epoch 35: total batches = 241150. train loss = 2.80, val loss = 2.80\n",
      "epoch 36: total batches = 248040. train loss = 2.78, val loss = 2.79\n",
      "epoch 37: total batches = 254930. train loss = 2.76, val loss = 2.77\n",
      "epoch 38: total batches = 261820. train loss = 2.75, val loss = 2.76\n",
      "epoch 39: total batches = 268710. train loss = 2.73, val loss = 2.73\n",
      "epoch 40: total batches = 275600. train loss = 2.71, val loss = 2.73\n",
      "epoch 41: total batches = 282490. train loss = 2.70, val loss = 2.71\n",
      "epoch 42: total batches = 289380. train loss = 2.68, val loss = 2.69\n",
      "epoch 43: total batches = 296270. train loss = 2.66, val loss = 2.67\n",
      "epoch 44: total batches = 303160. train loss = 2.65, val loss = 2.66\n",
      "epoch 45: total batches = 310050. train loss = 2.64, val loss = 2.65\n",
      "epoch 46: total batches = 316940. train loss = 2.62, val loss = 2.63\n",
      "epoch 47: total batches = 323830. train loss = 2.61, val loss = 2.63\n",
      "epoch 48: total batches = 330720. train loss = 2.59, val loss = 2.61\n",
      "epoch 49: total batches = 337610. train loss = 2.58, val loss = 2.59\n",
      "epoch 50: total batches = 344500. train loss = 2.56, val loss = 2.59\n",
      "epoch 51: total batches = 351390. train loss = 2.55, val loss = 2.58\n",
      "epoch 52: total batches = 358280. train loss = 2.54, val loss = 2.55\n",
      "epoch 53: total batches = 365170. train loss = 2.52, val loss = 2.55\n",
      "epoch 54: total batches = 372060. train loss = 2.51, val loss = 2.54\n",
      "epoch 55: total batches = 378950. train loss = 2.50, val loss = 2.54\n",
      "epoch 56: total batches = 385840. train loss = 2.49, val loss = 2.53\n",
      "epoch 57: total batches = 392730. train loss = 2.47, val loss = 2.50\n",
      "epoch 58: total batches = 399620. train loss = 2.46, val loss = 2.50\n",
      "epoch 59: total batches = 406510. train loss = 2.45, val loss = 2.49\n",
      "epoch 60: total batches = 413400. train loss = 2.43, val loss = 2.48\n",
      "epoch 61: total batches = 420290. train loss = 2.42, val loss = 2.46\n",
      "epoch 62: total batches = 427180. train loss = 2.41, val loss = 2.45\n",
      "epoch 63: total batches = 434070. train loss = 2.40, val loss = 2.44\n",
      "epoch 64: total batches = 440960. train loss = 2.39, val loss = 2.44\n",
      "epoch 65: total batches = 447850. train loss = 2.38, val loss = 2.43\n",
      "epoch 66: total batches = 454740. train loss = 2.37, val loss = 2.42\n",
      "epoch 67: total batches = 461630. train loss = 2.36, val loss = 2.41\n",
      "epoch 68: total batches = 468520. train loss = 2.34, val loss = 2.40\n",
      "epoch 69: total batches = 475410. train loss = 2.34, val loss = 2.39\n",
      "epoch 70: total batches = 482300. train loss = 2.32, val loss = 2.38\n",
      "epoch 71: total batches = 489190. train loss = 2.31, val loss = 2.37\n",
      "epoch 72: total batches = 496080. train loss = 2.30, val loss = 2.36\n",
      "epoch 73: total batches = 502970. train loss = 2.29, val loss = 2.36\n",
      "epoch 74: total batches = 509860. train loss = 2.28, val loss = 2.36\n",
      "epoch 75: total batches = 516750. train loss = 2.27, val loss = 2.34\n",
      "epoch 76: total batches = 523640. train loss = 2.26, val loss = 2.34\n",
      "epoch 77: total batches = 530530. train loss = 2.26, val loss = 2.33\n",
      "epoch 78: total batches = 537420. train loss = 2.25, val loss = 2.32\n",
      "epoch 79: total batches = 544310. train loss = 2.24, val loss = 2.31\n",
      "epoch 80: total batches = 551200. train loss = 2.23, val loss = 2.30\n",
      "epoch 81: total batches = 558090. train loss = 2.22, val loss = 2.30\n",
      "epoch 82: total batches = 564980. train loss = 2.21, val loss = 2.29\n",
      "epoch 83: total batches = 571870. train loss = 2.20, val loss = 2.28\n",
      "epoch 84: total batches = 578760. train loss = 2.19, val loss = 2.28\n",
      "epoch 85: total batches = 585650. train loss = 2.18, val loss = 2.27\n",
      "epoch 86: total batches = 592540. train loss = 2.17, val loss = 2.26\n",
      "epoch 87: total batches = 599430. train loss = 2.17, val loss = 2.25\n",
      "epoch 88: total batches = 606320. train loss = 2.16, val loss = 2.25\n",
      "epoch 89: total batches = 613210. train loss = 2.15, val loss = 2.24\n",
      "epoch 90: total batches = 620100. train loss = 2.14, val loss = 2.23\n",
      "epoch 91: total batches = 626990. train loss = 2.14, val loss = 2.23\n",
      "epoch 92: total batches = 633880. train loss = 2.13, val loss = 2.22\n",
      "epoch 93: total batches = 640770. train loss = 2.12, val loss = 2.21\n",
      "epoch 94: total batches = 647660. train loss = 2.11, val loss = 2.22\n",
      "epoch 95: total batches = 654550. train loss = 2.10, val loss = 2.20\n",
      "epoch 96: total batches = 661440. train loss = 2.10, val loss = 2.20\n",
      "epoch 97: total batches = 668330. train loss = 2.09, val loss = 2.19\n",
      "epoch 98: total batches = 675220. train loss = 2.08, val loss = 2.18\n",
      "epoch 99: total batches = 682110. train loss = 2.07, val loss = 2.18\n",
      "epoch 100: total batches = 689000. train loss = 2.07, val loss = 2.19\n",
      "epoch 101: total batches = 695890. train loss = 2.06, val loss = 2.17\n",
      "epoch 102: total batches = 702780. train loss = 2.06, val loss = 2.16\n",
      "epoch 103: total batches = 709670. train loss = 2.05, val loss = 2.16\n",
      "epoch 104: total batches = 716560. train loss = 2.04, val loss = 2.16\n",
      "epoch 105: total batches = 723450. train loss = 2.03, val loss = 2.15\n",
      "epoch 106: total batches = 730340. train loss = 2.03, val loss = 2.14\n",
      "epoch 107: total batches = 737230. train loss = 2.02, val loss = 2.13\n",
      "epoch 108: total batches = 744120. train loss = 2.01, val loss = 2.14\n",
      "epoch 109: total batches = 751010. train loss = 2.01, val loss = 2.12\n",
      "epoch 110: total batches = 757900. train loss = 2.00, val loss = 2.13\n",
      "epoch 111: total batches = 764790. train loss = 2.00, val loss = 2.13\n",
      "epoch 112: total batches = 771680. train loss = 1.99, val loss = 2.12\n",
      "epoch 113: total batches = 778570. train loss = 1.99, val loss = 2.11\n",
      "epoch 114: total batches = 785460. train loss = 1.98, val loss = 2.11\n",
      "epoch 115: total batches = 792350. train loss = 1.97, val loss = 2.10\n",
      "epoch 116: total batches = 799240. train loss = 1.97, val loss = 2.10\n",
      "epoch 117: total batches = 806130. train loss = 1.96, val loss = 2.09\n",
      "epoch 118: total batches = 813020. train loss = 1.96, val loss = 2.09\n",
      "epoch 119: total batches = 819910. train loss = 1.95, val loss = 2.09\n",
      "epoch 120: total batches = 826800. train loss = 1.95, val loss = 2.09\n",
      "End Training: total batches = 826800. train loss = 1.95, val loss = 2.08\n"
     ]
    }
   ],
   "source": [
    "print('Building training set ...')\n",
    "x, y = WindowModel.build_training_set(word_array)\n",
    "\n",
    "# shuffle and split 10% validation data\n",
    "x_shuf, y_shuf = sklearn.utils.shuffle(x, y, random_state=0)\n",
    "split = round(x_shuf.shape[0]*0.9)\n",
    "x_val, y_val = (x_shuf[split:, :], y_shuf[split:, :])\n",
    "x_train, y_train = (x[:split, :], y[:split, :])\n",
    "\n",
    "print('Training set built.')\n",
    "graph_params = {'batch_size': 32,\n",
    "                'vocab_size': np.max(x)+1,\n",
    "                'embed_size': 64,\n",
    "                'hid_size': 64,\n",
    "                'neg_samples': 64,\n",
    "                'learn_rate': 0.01,\n",
    "                'momentum': 0.9,\n",
    "                'embed_noise': 1,\n",
    "                'hid_noise': 0.3,\n",
    "                'optimizer': 'Momentum'}\n",
    "model = WindowModel(graph_params)\n",
    "print('Model built. Vocab size = {}. Document length = {} words.'\n",
    "      .format(np.max(x)+1, len(word_array)))\n",
    "\n",
    "print('Training ...')\n",
    "results = model.train(x_train, y_train, x_val, y_val, epochs=120)\n",
    "\n",
    "word_vector_embed = WordVector(results['embed_weights'], dictionary)\n",
    "word_vector_nce = WordVector(results['nce_weights'], dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100 Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'the', '\"', 'and', 'i', 'of', 'to', 'a', 'that', 'it', 'in', 'he', 'you', 'was', 'his', 'is', 'my', 'have', 'had', 'with', 'as', 'at', '?', 'for', 'which', 'we', 'but', 'be', 'not', 'me', 'this', 'there', 'upon', 'him', 'said', 'from', 'so', 'no', 'on', 'one', 'all', 'holmes', 'been', 'her', 'were', 'what', 'very', 'by', 'your', 'an', 'she', 'are', 'would', '!', 'man', 'out', 'could', 'then', 'if', 'our', 'up', 'when', 'has', 'do', 'will', \"'\", 'us', 'who', 'some', 'into', 'sir', 'now', 'see', 'down', 'they', 'or', 'should', 'little', 'mr', 'well', 'more', 'over', 'can', 'may', 'know', 'about', 'am', 'think', 'them', 'only', 'must', 'did', 'come', 'here', 'time', 'than', 'how', 'two', 'before']\n"
     ]
    }
   ],
   "source": [
    "print(word_vector_embed.words_in_range(0,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarities\n",
    "The model learns 2 word vector representations. \n",
    "1. The embedding vector from the one-hot input\n",
    "2. The vector from the hidden layer to the network output\n",
    "\n",
    "In general, the output layer vector seems to learn more meaningful vector representation of words. We quickly check the closest words (cosine similarity) to the word \"six\". Remember, this model had no human-labeled data or any data sources outside of the raw book text. The hidden layer to output matrix correctly finds that other numbers are most similar to \"six\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer: 10 closest words to: 'won't'\n",
      "['will', 'are', 'may', 'would', 'eventually', 'll', 'must', 'persuaded', 'sponged', 'should']\n",
      "\n",
      "Hidden-to-output layer: 10 closest words to: 'won't'\n",
      "['ll', 'cannot', \"can't\", 'must', \"don't\", 'strikes', 'pray', 'would', 'threw', 'shut']\n"
     ]
    }
   ],
   "source": [
    "word = \"won't\"\n",
    "print('Embedding layer: 10 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_embed.n_closest(word=word, num_closest=10, metric='cosine'))\n",
    "print()\n",
    "print('Hidden-to-output layer: 10 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_nce.n_closest(word=word, num_closest=10, metric='cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer: 10 closest words to: 'six'\n",
      "['folkestone', 's', \"'at\", 'all', 'tuesday', 'ninety', 'friendly', 'british', 'occasions', 'its']\n",
      "\n",
      "Hidden-to-output layer: 10 closest words to: 'six'\n",
      "['ten', 'seven', 'eleven', 'four', 'nine', 'five', 'two', 'cocking', 'three', 'typewriting']\n"
     ]
    }
   ],
   "source": [
    "word = \"six\"\n",
    "print('Embedding layer: 10 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_embed.n_closest(word=word, num_closest=10, metric='cosine'))\n",
    "print()\n",
    "print('Hidden-to-output layer: 10 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_nce.n_closest(word=word, num_closest=10, metric='cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer: 10 closest words to: 'london'\n",
      "['degrees', 'rapt', 'cranny', 'bohemia', 'risk', 'communication', '37', 'afghanistan', 'wayward', 'darker']\n",
      "\n",
      "Hidden-to-output layer: 10 closest words to: 'london'\n",
      "['devonshire', 'serpentine', 'trees', 'india', 'bristol', 'america', 'revellers', 'unclaimed', 'canada', 'wine']\n"
     ]
    }
   ],
   "source": [
    "word = \"london\"\n",
    "print('Embedding layer: 10 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_embed.n_closest(word=word, num_closest=10, metric='cosine'))\n",
    "print()\n",
    "print('Hidden-to-output layer: 10 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_nce.n_closest(word=word, num_closest=10, metric='cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# skipping first 100 words (i.e. 'the', 'if', 'and', '.', ',', ...) gives more\n",
    "# interesting visualization\n",
    "embed_2d, word_list = word_vector_nce.project_2d(100, 600) # t-sne projection\n",
    "reverse_dict = word_vector_nce.get_reverse_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['king', 'princess', 'tremendous', 'thing', 'immediately']\n",
      "['brook', 'summons', 'beaune', 'fund', 'gallows']\n"
     ]
    }
   ],
   "source": [
    "print(word_vector_embed.analogy('duke', 'king', 'princess', 5))\n",
    "print(word_vector_nce.analogy('duke', 'king', 'princess', 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accept', 'forget', 'uniforms', 'undeniable', 'craven']\n",
      "['accept', 'hide', 'forget', 'seize', 'seek']\n"
     ]
    }
   ],
   "source": [
    "print(word_vector_embed.analogy('remember', 'forget', 'accept', 5))\n",
    "print(word_vector_nce.analogy('remember', 'forget', 'accept', 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "passage = [x for x in map(lambda x: reverse_dict[x], word_array[12200:12300])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\" well,  it is just as i have been telling you,  mr.  sherlock holmes, \" said jabez wilson,  mopping his forehead\" i have a small pawnbroker's business at coburg square,  near the city.  it's not a very large affair,  and of late years it has not done more than just give me a living.  i used to be able to keep two assistants,  but now i only keep one and i would have a job to pay him but that he is willing to come for half wages\n"
     ]
    }
   ],
   "source": [
    "readable = ''\n",
    "for word in passage:\n",
    "    if word == '\"':\n",
    "        readable += word\n",
    "    elif word in ['?', '!', '.', ',']:\n",
    "        readable += word + ' '\n",
    "    else: \n",
    "        readable += ' ' + word\n",
    "print(readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = WindowModel.build_training_set(word_array[(12200-2):(12300+2)])\n",
    "y_hat = model.predict(x)\n",
    "passage_predict = [x for x in map(lambda x: reverse_dict[x], y_hat[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\" oh,  it was, .  you have to to you,  mr.  sherlock holmes, \" said he together,  with his armchair,  i was a small black was at his man,  and the moor. \" not a very good man,  and so the.  it is not been more.  you with you a man.  i began to be able to his,  years.  and when i should, ,  years you shall be a man to have.  sure if it is able to send from not than\n"
     ]
    }
   ],
   "source": [
    "readable = ''\n",
    "for word in passage_predict:\n",
    "    if word == '\"':\n",
    "        readable += word\n",
    "    elif word in ['?', '!', '.', ',']:\n",
    "        readable += word + ' '\n",
    "    else: \n",
    "        readable += ' ' + word\n",
    "print(readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
