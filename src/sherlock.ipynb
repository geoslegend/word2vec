{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Word Vectors from Sherlock Holmes Series\n",
    "Patrick Coady (pcoady@alum.mit.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordvector import WordVector\n",
    "from windowmodel import WindowModel\n",
    "import docload\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Books, Build Dictionary & Convert Books to Integer Vector\n",
    "Start with these 3 books (all written by Sir Arthor Conan Doyle):\n",
    "1. The Adventures of Sherlock Holmes\n",
    "2. The Hound of the Baskervilles\n",
    "3. The Sign of the Four\n",
    "\n",
    "Load the books and build a dictionary of all unique words. The dictionary maps each unique word to an integer. All words are converted to lower case. And punctuation are treated as words (i.e. \" , . ? and !). If the size of the book vocabulary exceeds the pre-set limit (**vocab_size**), then the most infrequent words are mapped to the last integer in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded and processed: 24080 lines, 244986 words.\n"
     ]
    }
   ],
   "source": [
    "files = ['../data/adventures_of_sherlock_holmes.txt',\n",
    "        '../data/hound_of_the_baskervilles.txt',\n",
    "        '../data/sign_of_the_four.txt']\n",
    "word_array, dictionary, num_lines, num_words = docload.build_word_array(\n",
    "    files, vocab_size=50000, gutenberg=True)\n",
    "\n",
    "print('Document loaded and processed: {} lines, {} words.'\n",
    "      .format(num_lines, num_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Architecture\n",
    "![](notebook_images/NN_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training set ...\n",
      "Training set built.\n",
      "Model built. Vocab size = 11750. Document length = 244986 words.\n",
      "Training ...\n",
      "End Training: total batches = 826800. train loss = 1.93, val loss = 2.06\n"
     ]
    }
   ],
   "source": [
    "print('Building training set ...')\n",
    "x, y = WindowModel.build_training_set(word_array)\n",
    "\n",
    "# shuffle and split 10% validation data\n",
    "x_shuf, y_shuf = sklearn.utils.shuffle(x, y, random_state=0)\n",
    "split = round(x_shuf.shape[0]*0.9)\n",
    "x_val, y_val = (x_shuf[split:, :], y_shuf[split:, :])\n",
    "x_train, y_train = (x[:split, :], y[:split, :])\n",
    "\n",
    "print('Training set built.')\n",
    "graph_params = {'batch_size': 32,\n",
    "                'vocab_size': np.max(x)+1,\n",
    "                'embed_size': 64,\n",
    "                'hid_size': 64,\n",
    "                'neg_samples': 64,\n",
    "                'learn_rate': 0.01,\n",
    "                'momentum': 0.9,\n",
    "                'embed_noise': 0.1,\n",
    "                'hid_noise': 0.1,\n",
    "                'optimizer': 'Momentum'}\n",
    "model = WindowModel(graph_params)\n",
    "print('Model built. Vocab size = {}. Document length = {} words.'\n",
    "      .format(np.max(x)+1, len(word_array)))\n",
    "\n",
    "print('Training ...')\n",
    "results = model.train(x_train, y_train, x_val, y_val, epochs=120, verbose=False)\n",
    "\n",
    "word_vector_embed = WordVector(results['embed_weights'], dictionary)\n",
    "word_vector_nce = WordVector(results['nce_weights'], dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100 Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'the', '\"', 'and', 'i', 'of', 'to', 'a', 'that', 'it', 'in', 'he', 'you', 'was', 'his', 'is', 'my', 'have', 'had', 'with', 'as', 'at', '?', 'for', 'which', 'we', 'but', 'be', 'not', 'me', 'this', 'there', 'upon', 'him', 'said', 'from', 'so', 'no', 'on', 'one', 'all', 'holmes', 'been', 'her', 'were', 'what', 'very', 'by', 'your', 'an', 'she', 'are', '!', 'would', 'man', 'out', 'could', 'then', 'if', 'our', 'up', 'when', 'has', 'do', 'will', \"'\", 'us', 'who', 'some', 'into', 'sir', 'now', 'see', 'down', 'they', 'or', 'should', 'little', 'mr', 'well', 'more', 'over', 'can', 'may', 'know', 'about', 'am', 'them', 'think', 'only', 'must', 'did', 'here', 'come', 'time', 'than', 'how', 'two', 'before']\n"
     ]
    }
   ],
   "source": [
    "print(word_vector_embed.words_in_range(0,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarities\n",
    "The model learns 2 word vector representations. \n",
    "1. The embedding vector from the one-hot input\n",
    "2. The vector from the hidden layer to the network output\n",
    "\n",
    "In general, the output layer vector seems to learn more meaningful vector representation of words. We quickly check the closest words (cosine similarity) to the word \"six\". Remember, this model had no human-labeled data or any data sources outside of the raw book text. The hidden layer to output matrix correctly finds that other numbers are most similar to \"six\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer: 8 closest words to: 'won't'\n",
      "['would', 'will', 'clogs', 'grip', 'indeed', 'refusal', 'ryder', 'unraveling'] \n",
      "\n",
      "Hidden-to-output layer: 8 closest words to: 'won't'\n",
      "['alternately', 'll', 'would', 'may', 'pray', 'must', \"didn't\", \"can't\"]\n"
     ]
    }
   ],
   "source": [
    "word = \"won't\"\n",
    "print('Embedding layer: 8 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_embed.n_closest(word=word, num_closest=8, metric='cosine'), '\\n')\n",
    "print('Hidden-to-output layer: 8 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_nce.n_closest(word=word, num_closest=8, metric='cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer: 8 closest words to: 'running'\n",
      "[\"'i've\", 'glances', \"wi'\", 'delved', 'swiftly', 'natur', 'chesterfield', 'lust'] \n",
      "\n",
      "Hidden-to-output layer: 8 closest words to: 'running'\n",
      "['tugging', 'sharpened', 'wriggled', 'rushing', 'porters', 'sidled', 'trailing', \"'singular\"]\n"
     ]
    }
   ],
   "source": [
    "word = \"running\"\n",
    "print('Embedding layer: 8 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_embed.n_closest(word=word, num_closest=8, metric='cosine'), '\\n')\n",
    "print('Hidden-to-output layer: 8 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_nce.n_closest(word=word, num_closest=8, metric='cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer: 8 closest words to: 'seven'\n",
      "['ticking', 'drift', 'obstacles', 'sundials', 'exacted', 'ten', 'trip', 'rained'] \n",
      "\n",
      "Hidden-to-output layer: 8 closest words to: 'seven'\n",
      "['five', 'eight', 'four', 'peace', 'six', 'many', 'twelve', 'greater']\n"
     ]
    }
   ],
   "source": [
    "word = \"seven\"\n",
    "print('Embedding layer: 8 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_embed.n_closest(word=word, num_closest=8, metric='cosine'), '\\n')\n",
    "print('Hidden-to-output layer: 8 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_nce.n_closest(word=word, num_closest=8, metric='cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# skipping first 100 words (i.e. 'the', 'if', 'and', '.', ',', ...) gives more\n",
    "# interesting visualization\n",
    "embed_2d, word_list = word_vector_nce.project_2d(100, 600) # t-sne projection\n",
    "reverse_dict = word_vector_nce.get_reverse_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['king', 'lonely', 'corps', 'saints', 'princess']\n",
      "['princess', 'undertaking', 'reincarnation', \"'77\", 'crumbling']\n"
     ]
    }
   ],
   "source": [
    "print(word_vector_embed.analogy('duke', 'king', 'princess', 5))\n",
    "print(word_vector_nce.analogy('duke', 'king', 'princess', 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accept', 'forget', 'strategically', 'particulars', 'morstan']\n",
      "['accept', 'commence', 'playing', 'entering', 'rejoin']\n"
     ]
    }
   ],
   "source": [
    "print(word_vector_embed.analogy('remember', 'forget', 'accept', 5))\n",
    "print(word_vector_nce.analogy('remember', 'forget', 'accept', 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "passage = [x for x in map(lambda x: reverse_dict[x], word_array[12200:12300])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\" well,  it is just as i have been telling you,  mr.  sherlock holmes, \" said jabez wilson,  mopping his forehead\" i have a small pawnbroker's business at coburg square,  near the city.  it's not a very large affair,  and of late years it has not done more than just give me a living.  i used to be able to keep two assistants,  but now i only keep one and i would have a job to pay him but that he is willing to come for half wages\n"
     ]
    }
   ],
   "source": [
    "readable = ''\n",
    "for word in passage:\n",
    "    if word == '\"':\n",
    "        readable += word\n",
    "    elif word in ['?', '!', '.', ',']:\n",
    "        readable += word + ' '\n",
    "    else: \n",
    "        readable += ' ' + word\n",
    "print(readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = WindowModel.build_training_set(word_array[(12200-2):(12300+2)])\n",
    "y_hat = model.predict(x, 120)\n",
    "passage_predict = [x for x in map(lambda x: reverse_dict[x], y_hat[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\" yes,  that was,  as i have been with you,  mr.  sherlock holmes, \" said holmes. ,  with his. ,  i am a great blue.  at baskerville square,  with the house.  did not a little white time,  and of a.  it is not been more than i let me a week.  i came to be able to have a hours,  and as the shall an,  which i should have been time and get.  but what it is able to be in an was\n"
     ]
    }
   ],
   "source": [
    "readable = ''\n",
    "for word in passage_predict:\n",
    "    if word == '\"':\n",
    "        readable += word\n",
    "    elif word in ['?', '!', '.', ',']:\n",
    "        readable += word + ' '\n",
    "    else: \n",
    "        readable += ' ' + word\n",
    "print(readable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
