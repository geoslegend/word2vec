{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Word Vectors from Sherlock Holmes Books\n",
    "*Patrick Coady (pcoady@alum.mit.edu)*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordvector import WordVector\n",
    "from windowmodel import WindowModel\n",
    "import docload\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Books, Build Dictionary & Convert Books to Integer Vector\n",
    "Start with these 3 books (all written by Sir Arthor Conan Doyle):\n",
    "1. The Adventures of Sherlock Holmes\n",
    "2. The Hound of the Baskervilles\n",
    "3. The Sign of the Four\n",
    "\n",
    "Load the books and build a dictionary of all unique words. The dictionary maps each unique word to an integer. All words are converted to lower case. And punctuation are treated as words (i.e. \" , . ? and !). If the size of the book vocabulary exceeds the pre-set limit (**vocab_size**), then the most infrequent words are mapped to the last integer in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded and processed: 24080 lines, 247812 words.\n"
     ]
    }
   ],
   "source": [
    "files = ['../data/adventures_of_sherlock_holmes.txt',\n",
    "        '../data/hound_of_the_baskervilles.txt',\n",
    "        '../data/sign_of_the_four.txt']\n",
    "word_array, dictionary, num_lines, num_words = docload.build_word_array(\n",
    "    files, vocab_size=50000, gutenberg=True)\n",
    "\n",
    "print('Document loaded and processed: {} lines, {} words.'\n",
    "      .format(num_lines, num_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Architecture\n",
    "![](notebook_images/NN_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training set ...\n",
      "Training set built.\n",
      "Model built. Vocab size = 11756. Document length = 247812 words.\n",
      "Training ...\n",
      "End Training: total batches = 836280. train loss = 1.42, val loss = 1.66\n"
     ]
    }
   ],
   "source": [
    "print('Building training set ...')\n",
    "x, y = WindowModel.build_training_set(word_array)\n",
    "\n",
    "# shuffle and split 10% validation data\n",
    "x_shuf, y_shuf = sklearn.utils.shuffle(x, y, random_state=0)\n",
    "split = round(x_shuf.shape[0]*0.9)\n",
    "x_val, y_val = (x_shuf[split:, :], y_shuf[split:, :])\n",
    "x_train, y_train = (x[:split, :], y[:split, :])\n",
    "\n",
    "print('Training set built.')\n",
    "graph_params = {'batch_size': 32,\n",
    "                'vocab_size': np.max(x)+1,\n",
    "                'embed_size': 64,\n",
    "                'hid_size': 64,\n",
    "                'neg_samples': 64,\n",
    "                'learn_rate': 0.01,\n",
    "                'momentum': 0.9,\n",
    "                'embed_noise': 0.1,\n",
    "                'hid_noise': 0.3,\n",
    "                'optimizer': 'Momentum'}\n",
    "model = WindowModel(graph_params)\n",
    "print('Model built. Vocab size = {}. Document length = {} words.'\n",
    "      .format(np.max(x)+1, len(word_array)))\n",
    "\n",
    "print('Training ...')\n",
    "results = model.train(x_train, y_train, x_val, y_val, epochs=120, verbose=False)\n",
    "\n",
    "word_vector_embed = WordVector(results['embed_weights'], dictionary)\n",
    "word_vector_nce = WordVector(results['nce_weights'], dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100 Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'the', '\"', 'and', 'i', 'of', 'to', 'a', 'that', 'it', 'in', 'he', 'you', 'was', '-', 'his', 'is', 'my', 'have', 'had', 'with', 'as', 'at', '?', 'for', 'which', 'we', 'but', 'be', 'not', 'me', 'this', 'there', 'upon', 'him', 'said', 'from', 'so', 'no', 'on', 'one', 'all', 'holmes', 'been', 'her', 'were', 'what', 'very', 'by', 'your', 'an', 'she', 'are', 'would', '!', 'man', 'out', 'could', 'then', 'if', 'our', 'up', 'when', 'has', 'do', 'will', \"'\", 'us', 'who', 'some', 'into', 'sir', 'now', 'see', 'down', 'or', 'they', 'should', 'little', 'mr', 'well', 'more', 'over', 'can', 'may', 'know', 'about', 'am', 'them', 'think', 'only', 'must', ';', 'did', 'here', 'come', 'time', 'than', 'how']\n"
     ]
    }
   ],
   "source": [
    "print(word_vector_embed.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarities\n",
    "The model learns 2 word vector representations. \n",
    "1. The embedding vector from the one-hot input\n",
    "2. The vector from the hidden layer to the network output\n",
    "\n",
    "In general, the output layer vector seems to learn more meaningful vector representation of words. We quickly check the closest words (cosine similarity) to the word \"seven\". Remember, this model had no human-labeled data or any data sources outside of the raw book text. The hidden layer to output matrix correctly finds that other numbers are most similar to \"seven\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer: 8 closest words to: 'seven'\n",
      "['two', 'charing', 'eight', 'political', 'they', 'four', \"king's\", 'three'] \n",
      "\n",
      "Hidden-to-output layer: 8 closest words to: 'seven'\n",
      "['ten', 'five', 'eleven', 'eight', 'nine', 'six', 'fifty', 'twelve']\n"
     ]
    }
   ],
   "source": [
    "word = \"seven\"\n",
    "print('Embedding layer: 8 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_embed.n_closest(word=word, num_closest=8, metric='cosine'), '\\n')\n",
    "print('Hidden-to-output layer: 8 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_nce.n_closest(word=word, num_closest=8, metric='cosine'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Performance of *embed_weights* vs. *nce_weights*  \n",
    "\n",
    "From a qualitative perspective, the *nce_weights* consistently give more meaningful results when asking for similar words. Although, they both do OK on suggesting similar words to \"seven\". Also, for the analogy task (e.g. A is to B, as C is to ?) the *nce_weights* give more \"sensible\" results.  \n",
    "\n",
    "Clearly, the *embed_weights* are learning something. These weights are the first stage in the model, and the model wouldn't perform if they were learning nonsense. \n",
    "\n",
    "Cosine similarity is used as a distance metric for running similarity and analogy tasks. It might be interesting to experiment with other distance measures. A quick look at Euclidean distance was not promising. The code supports 15+ different distance metrics from *scipy.spatial.distance*, some experimentation here might be interesting.  \n",
    "\n",
    "That said, to avoid clutter, the rest of this notebook will focus on the word vectors from the *nce_weights* matrix and use cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 closest words to: 'laughing'\n",
      "['pensively', 'smiling', 'yawning', 'chuckling', 'georgia', 'mcmurdo', 'brusquely', 'lightly']\n"
     ]
    }
   ],
   "source": [
    "word = \"laughing\"\n",
    "print('8 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_nce.n_closest(word=word, num_closest=8, metric='cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 closest words to: 'mr'\n",
      "['mrs', \"'mr\", 'dr', 'l', 'etc', 'haggard', 'h', '1890']\n"
     ]
    }
   ],
   "source": [
    "word = \"mr\"\n",
    "print('8 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_nce.n_closest(word=word, num_closest=8, metric='cosine'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogies  \n",
    "\n",
    "Because words are represented as vectors, it is interesting to try vector addition to predict the 4th word in an analogy. The premise being that A is to B as C is to D can be represented as:  $\\mathbf{x_d}=\\mathbf{x_b}-\\mathbf{x_a}+\\mathbf{x_c}$.  \n",
    "\n",
    "![](notebook_images/analogies.png)\n",
    "\n",
    "And it does indeed work. The first example predicts that \"**had** is to **has** as **was** is to **is**\". So the word vectors can translate between past and present tense. Note, that it predicts words that are already given in the analogy. In 64-dimensional vector space, it is hard to get too far away from $\\mathbf{x_a}$, $\\mathbf{x_b}$ and $\\mathbf{x_c}$ in the above equation.  \n",
    "\n",
    "In the 2nd example, the vector addition can correctly translate between the singular and plural form of a noun. With the fun example being, \"**boot** is to **boots** as **arm** is to (**arms** or **limbs**)\".  \n",
    "\n",
    "(I am generously by giving credit for making a correct guess in the top-5. I don't feel too badly about this, the training set is fairly small.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['was', 'is', 'lies', 'has', 'becomes']\n"
     ]
    }
   ],
   "source": [
    "print(word_vector_nce.analogy('had', 'has', 'was', 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boots', 'faces', 'arm', 'shoulder', 'finger']\n"
     ]
    }
   ],
   "source": [
    "print(word_vector_nce.analogy('boot', 'boots', 'arm', 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Replacement Words in a Passage\n",
    "\n",
    "For fun, I took a random 200 word passage and used the network to make predictions to replace all the words. The results has some semblance of grammar, but is mostly nonsensical. This is to be expected, the model only uses the 2 preceding and 2 following words to make predictions. A Recurrent NN is a more appropriate tool for this, but here it is anyway:\n",
    "\n",
    "#### Original Passage\n",
    "\n",
    "**well,  it is just as i have been telling you,  mr.  sherlock holmes, \" said jabez wilson,  mopping his forehead\" i have a small pawnbroker's business at coburg square,  near the city.**\n",
    "\n",
    "#### Reconstructed Passage\n",
    "\n",
    "**oh,  it was,  as i have been told you,  mr.  sherlock holmes, \" said sherlock wilson,  upon his? \" i am a small public business at coburg square,  with the time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grab 100 word passage from book\n",
    "reverse_dict = word_vector_nce.get_reverse_dict()\n",
    "passage = [x for x in map(lambda x: reverse_dict[x], word_array[12200:12300])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",  at eleven o'clock,  to duncan ross,  at the offices of the league,  7 pope's court,  fleet street. \"\" what on earth does this mean? \" i ejaculated after i had twice read over the extraordinary announcement.  holmes chuckled and wriggled in his chair,  as was his habit when in high spirits. \" it is a little off the beaten track,  isn't it? \" said he. \" and now,  mr.  wilson,  off you go at scratch and tell us all about yourself,  your\n"
     ]
    }
   ],
   "source": [
    "# print passage with some crude formatting (e.g. space after comma)\n",
    "readable = ''\n",
    "for word in passage:\n",
    "    if word == '\"':\n",
    "        readable += word\n",
    "    elif word in ['?', '!', '.', ',']:\n",
    "        readable += word + ' '\n",
    "    else: \n",
    "        readable += ' ' + word\n",
    "print(readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use model to replace words in original passage with predicted words\n",
    "# need to grab 2 words before and after passage\n",
    "x, y = WindowModel.build_training_set(word_array[(12200-2):(12300+2)])\n",
    "y_hat = model.predict(x, 120)\n",
    "passage_predict = [x for x in map(lambda x: reverse_dict[x], y_hat[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",  at those o'clock,  to thank us,  at the top of the matter,  for - court,  baker street. \"\" what on which could this fellow? \" i was that i had been turned over the old attention.  holmes opened and himself in his face,  which in his voice when in seven face. \" it is so hurried light the very street,  thank he? \" said he. \" and well,  mr.  holmes,  did you go at once and tell you all that me,  your\n"
     ]
    }
   ],
   "source": [
    "# print predicted passage\n",
    "readable = ''\n",
    "for word in passage_predict:\n",
    "    if word == '\"':\n",
    "        readable += word\n",
    "    elif word in ['?', '!', '.', ',']:\n",
    "        readable += word + ' '\n",
    "    else: \n",
    "        readable += ' ' + word\n",
    "print(readable)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "I hope you found this interesting or even useful. It is amazing that with a few afternoons you can demonstrate results like this. And all using free, open-source tools. \n",
    "\n",
    "I've been pleased with TensorFlow. I haven't benchmarked for performance, but the API is intuitive and you can get something working quickly. I was previously using Keras to build NN models. I decided to try TensorFlow to have more flexibility to try ideas and also to understand what was going on behind the scenes.\n",
    "\n",
    "The code in the 3 python modules is intended to be easily understood and reused. I encourage people to run the model on different books or documents - I'd love to hear your results. I welcome suggestions and even Pull Requests with fixes or improvements.\n",
    "\n",
    "Next I plan to train a RNN on the same data set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
