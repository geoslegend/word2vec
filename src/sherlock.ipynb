{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Word Vectors from Sherlock Holmes Series\n",
    "Patrick Coady (pcoady@alum.mit.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordvector import WordVector\n",
    "from windowmodel import WindowModel\n",
    "import docload\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Books, Build Dictionary & Convert Books to Integer Vector\n",
    "Start with these 3 books (all written by Sir Arthor Conan Doyle):\n",
    "1. The Adventures of Sherlock Holmes\n",
    "2. The Hound of the Baskervilles\n",
    "3. The Sign of the Four\n",
    "\n",
    "Load the books and build a dictionary of all unique words. The dictionary maps each unique word to an integer. All words are converted to lower case. And punctuation are treated as words (i.e. \" , . ? and !). If the size of the book vocabulary exceeds the pre-set limit (**vocab_size**), then the most infrequent words are mapped to the last integer in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded and processed: 24080 lines, 244986 words.\n"
     ]
    }
   ],
   "source": [
    "files = ['../data/adventures_of_sherlock_holmes.txt',\n",
    "        '../data/hound_of_the_baskervilles.txt',\n",
    "        '../data/sign_of_the_four.txt']\n",
    "word_array, dictionary, num_lines, num_words = docload.build_word_array(\n",
    "    files, vocab_size=50000, gutenberg=True)\n",
    "\n",
    "print('Document loaded and processed: {} lines, {} words.'\n",
    "      .format(num_lines, num_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Architecture\n",
    "![](notebook_images/NN_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Building training set ...')\n",
    "x, y = WindowModel.build_training_set(word_array)\n",
    "\n",
    "# shuffle and split 10% validation data\n",
    "x_shuf, y_shuf = sklearn.utils.shuffle(x, y, random_state=0)\n",
    "split = round(x_shuf.shape[0]*0.9)\n",
    "x_val, y_val = (x_shuf[split:, :], y_shuf[split:, :])\n",
    "x_train, y_train = (x[:split, :], y[:split, :])\n",
    "\n",
    "print('Training set built.')\n",
    "graph_params = {'batch_size': 32,\n",
    "                'vocab_size': np.max(x)+1,  # +1 for unknown word dictionary entry\n",
    "                'embed_size': 16,\n",
    "                'hid_size': 16,\n",
    "                'learn_rate': 0.01,\n",
    "                'momentum': 0.9}\n",
    "model = WindowModel(graph_params)\n",
    "print('Model built. Vocab size = {}. Document length = {} words.'\n",
    "      .format(np.max(x)+1, len(word_array)))\n",
    "\n",
    "print('Training ...')\n",
    "results = model.train(x_train, y_train, x_val, y_val, epochs=120)\n",
    "\n",
    "word_vector_embed = WordVector(results['embed_weights'], dictionary)\n",
    "word_vector_nce = WordVector(results['nce_weights'], dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100 Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(word_vector_embed.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarities\n",
    "The model learns 2 word vector representations. \n",
    "1. The embedding vector from the one-hot input\n",
    "2. The vector from the hidden layer to the network output\n",
    "\n",
    "In general, the output layer vector seems to learn more meaningful vector representation of words. We quickly check the closest words (cosine similarity) to the word \"six\". Remember, this model had no human-labeled data or any data sources outside of the raw book text. The hidden layer to output matrix correctly finds that other numbers are most similar to \"six\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word = \"quickly\"\n",
    "print('Embedding layer: 10 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_embed.n_closest(word=word, num_closest=10, metric='cosine'))\n",
    "print()\n",
    "print('Hidden-to-output layer: 10 closest words to:', \"'\" + word + \"'\")\n",
    "print(word_vector_nce.n_closest(word=word, num_closest=10, metric='cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# skipping first 100 words (i.e. 'the', 'if', 'and', '.', ',', ...) gives more\n",
    "# interesting visualization\n",
    "embed_2d, word_list = word_vector_nce.project_2d(100, 600) # t-sne projection\n",
    "reverse_dict = word_vector_nce.get_reverse_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (8, 6)\n",
    "# minx, maxx, miny, maxy = (-20, 0, -20, 10)\n",
    "# plt.ylim(miny, maxy)\n",
    "# plt.xlim(minx, maxx)\n",
    "plt.scatter(embed_2d[:,0], embed_2d[:,1])\n",
    "for i in range(500):\n",
    "    plt.text(embed_2d[i,0], embed_2d[i,1], reverse_dict[i], clip_on=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_vector_nce.analogy('london', 'england', 'scotland', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_predict = x[:100,:]\n",
    "y_hat = model.predict(x_predict, 120)\n",
    "np.sum(y_hat[0]-y[:100,0]==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
