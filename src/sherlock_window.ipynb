{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Learning word vectors from Sherlock Holmes series\n",
    "\n",
    "Patrick Coady (pcoady@alum.mit.edu)\n",
    "\"\"\"\n",
    "\n",
    "from wordvector import WordVector\n",
    "from windowmodel import WindowModel\n",
    "import docload\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded and processed: 24080 lines, 244986 words.\n"
     ]
    }
   ],
   "source": [
    "# UNCOMMENT below to load and process a document for first time\n",
    "\n",
    "files = ['../data/adventures_of_sherlock_holmes.txt',\n",
    "        '../data/hound_of_the_baskervilles.txt',\n",
    "        '../data/sign_of_the_four.txt']\n",
    "word_array, dictionary, num_lines, num_words = docload.build_word_array(\n",
    "    files, vocab_size=50000, gutenberg=True)\n",
    "# save processed book for quick future load\n",
    "docload.save_word_array('../data/aofsh', word_array, dictionary)\n",
    "\n",
    "print('Document loaded and processed: {} lines, {} words.'\n",
    "      .format(num_lines, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## UNCOMMENT below to load previously processed book\n",
    "## aofsh = previously processed Adventures of Sherlock Holmes\n",
    "\n",
    "# word_array, dictionary = docload.load_word_array('../data/aofsh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training set ...\n",
      "Training set built.\n",
      "Model built. Vocab size = 11750. Document length = 244986 words.\n",
      "Training ...\n",
      "epoch 1: total batches = 6890. train loss = 113.16, val loss = 77.61\n",
      "epoch 2: total batches = 13780. train loss = 62.71, val loss = 51.11\n",
      "epoch 3: total batches = 20670. train loss = 43.10, val loss = 35.76\n",
      "epoch 4: total batches = 27560. train loss = 31.17, val loss = 26.08\n",
      "epoch 5: total batches = 34450. train loss = 23.10, val loss = 19.96\n",
      "epoch 6: total batches = 41340. train loss = 17.65, val loss = 15.01\n",
      "epoch 7: total batches = 48230. train loss = 13.76, val loss = 12.38\n",
      "epoch 8: total batches = 55120. train loss = 11.06, val loss = 9.80\n",
      "epoch 9: total batches = 62010. train loss = 9.09, val loss = 8.28\n",
      "epoch 10: total batches = 68900. train loss = 7.76, val loss = 7.18\n",
      "epoch 11: total batches = 75790. train loss = 6.82, val loss = 6.48\n",
      "epoch 12: total batches = 82680. train loss = 6.19, val loss = 5.99\n",
      "epoch 13: total batches = 89570. train loss = 5.79, val loss = 5.66\n",
      "epoch 14: total batches = 96460. train loss = 5.54, val loss = 5.47\n",
      "epoch 15: total batches = 103350. train loss = 5.37, val loss = 5.31\n",
      "epoch 16: total batches = 110240. train loss = 5.25, val loss = 5.22\n",
      "epoch 17: total batches = 117130. train loss = 5.17, val loss = 5.17\n",
      "epoch 18: total batches = 124020. train loss = 5.12, val loss = 5.12\n",
      "epoch 19: total batches = 130910. train loss = 5.09, val loss = 5.10\n",
      "epoch 20: total batches = 137800. train loss = 5.07, val loss = 5.09\n",
      "epoch 21: total batches = 144690. train loss = 5.05, val loss = 5.07\n",
      "epoch 22: total batches = 151580. train loss = 5.04, val loss = 5.06\n",
      "epoch 23: total batches = 158470. train loss = 5.03, val loss = 5.08\n",
      "epoch 24: total batches = 165360. train loss = 5.03, val loss = 5.06\n",
      "epoch 25: total batches = 172250. train loss = 5.03, val loss = 5.07\n",
      "epoch 26: total batches = 179140. train loss = 5.03, val loss = 5.07\n",
      "epoch 27: total batches = 186030. train loss = 5.02, val loss = 5.08\n",
      "epoch 28: total batches = 192920. train loss = 5.02, val loss = 5.09\n",
      "epoch 29: total batches = 199810. train loss = 5.02, val loss = 5.08\n",
      "epoch 30: total batches = 206700. train loss = 5.02, val loss = 5.09\n"
     ]
    }
   ],
   "source": [
    "print('Building training set ...')\n",
    "x, y = WindowModel.build_training_set(word_array)\n",
    "\n",
    "# shuffle and split 10% validation data\n",
    "x, y = shuffle(x, y, random_state=0)\n",
    "split = round(x.shape[0]*0.9)\n",
    "x_val, y_val = (x[split:, :], y[split:, :])\n",
    "x, y = (x[:split, :], y[:split, :])\n",
    "\n",
    "print('Training set built.')\n",
    "graph_params = {'batch_size': 32,\n",
    "                'vocab_size': np.max(x)+1,\n",
    "                'embed_size': 64,\n",
    "                'hid_size': 64,\n",
    "                'neg_samples': 64,\n",
    "                'learn_rate': 0.002,\n",
    "                'name': 'sherlock'}  # name for model save\n",
    "model = WindowModel(graph_params)\n",
    "print('Model built. Vocab size = {}. Document length = {} words.'\n",
    "      .format(np.max(x)+1, len(word_array)))\n",
    "\n",
    "print('Training ...')\n",
    "results = model.train(x, y, x_val, y_val, epochs=30)\n",
    "\n",
    "word_vector_embed = WordVector(results['embed_weights'], dictionary)\n",
    "word_vector_nce = WordVector(results['nce_weights'], dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 most common words\n",
      "[',', '.', 'the', '\"', 'and', 'i', 'of', 'to', 'a', 'that', 'it', 'in', 'he', 'you', 'was', 'his', 'is', 'my', 'have', 'had', 'with', 'as', 'at', '?', 'for', 'which', 'we', 'but', 'be', 'not', 'me', 'this', 'there', 'upon', 'him', 'said', 'from', 'so', 'no', 'on', 'one', 'all', 'holmes', 'been', 'her', 'were', 'what', 'very', 'by', 'your', 'an', 'she', 'are', 'would', '!', 'man', 'out', 'could', 'then', 'if', 'our', 'up', 'when', 'has', 'do', 'will', \"'\", 'us', 'who', 'some', 'into', 'sir', 'now', 'see', 'down', 'they', 'or', 'should', 'little', 'mr', 'well', 'more', 'over', 'can', 'may', 'know', 'about', 'am', 'think', 'them', 'only', 'must', 'did', 'here', 'come', 'time', 'than', 'how', 'two', 'before']\n"
     ]
    }
   ],
   "source": [
    "print('100 most common words')\n",
    "print(word_vector_embed.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 closest words to: \" think \". (based on cosine similarity)\n",
      "['been', 'einen', 'sunlight', 'chesterfield', 'imply', 'desperately', 'burglary', 'island', 'objections', 'contact']\n"
     ]
    }
   ],
   "source": [
    "word = \"think\"\n",
    "print('10 closest words to: \"', word, '\". (based on cosine similarity)')\n",
    "print(word_vector_embed.n_closest(word=word, num_closest=10, metric='cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 closest words to: \" who \". (based on cosine similarity)\n",
      "['farintosh', 'abrupt', 'mastery', 'scents', 'firelight', 'effected', 'texas', 'peeped', 'shed', \"arthur's\"]\n"
     ]
    }
   ],
   "source": [
    "word = \"who\"\n",
    "print('10 closest words to: \"', word, '\". (based on cosine similarity)')\n",
    "print(word_vector_nce.n_closest(word=word, num_closest=10, metric='cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_vector2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2c93079c9a27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membed_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vector2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreverse_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vector2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reverse_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mminx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# plt.ylim(miny, maxy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# plt.xlim(minx, maxx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_vector2' is not defined"
     ]
    }
   ],
   "source": [
    "embed_2d, word_list = word_vector2.project_2d(200, 700)\n",
    "reverse_dict = word_vector2.get_reverse_dict()\n",
    "minx, maxx, miny, maxy = (-10, 0, 20, 30)\n",
    "# plt.ylim(miny, maxy)\n",
    "# plt.xlim(minx, maxx)\n",
    "plt.scatter(embed_2d[:,0], embed_2d[:,1])\n",
    "for i in range(500):\n",
    "    plt.text(embed_2d[i,0], embed_2d[i,1], reverse_dict[i], clip_on=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_vector_embed.analogy('gentleman', 'lady', 'lord', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([[9,62,711,51]])\n",
    "y_hat = model.predict(x, epoch=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  27,   10,   16, 1159,    9,   62,   51,  711,   51,   84], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_array[9000:9010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
